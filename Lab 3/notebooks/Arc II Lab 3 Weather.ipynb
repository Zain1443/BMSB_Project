{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Accuracy\n",
    "\n",
    "This notebook creates a pipeline which determines both the most accurate interpolation method for a series of weather station locations within Minnesota and the accuracy of the point data through comparison of known elevation data. The data is then stored in a file geodatabase and moved to an .sde database for creating real-time web maps in ArcGIS Online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add and organize weather data from SDE database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import arcpy\n",
    "import random\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import pandas as pd\n",
    "import arcgis\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish variable for the date\n",
    "today = str(date.today()).replace('-','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.dirname(arcpy.mp.ArcGISProject('CURRENT').filePath)\n",
    "os.chdir(file_path)\n",
    "\n",
    "arcpy.env.workspace = file_path\n",
    "spatial_ref = arcpy.SpatialReference(4326)\n",
    "\n",
    "# Establish variables for project and map\n",
    "project = arcpy.mp.ArcGISProject(\"CURRENT\")\n",
    "m = project.listMaps(\"Map\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Wednesday, April 10, 2024 7:39:03 PM\",\"WARNING 000258: Output C:\\\\Users\\\\15612\\\\Documents\\\\Arc II\\\\Lab 3\\\\Lab 3\\\\PostgreSQL-35-gis5572(postgres).sde already exists\",\"Succeeded at Wednesday, April 10, 2024 7:39:31 PM (Elapsed Time: 28.22 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'C:\\\\Users\\\\15612\\\\Documents\\\\Arc II\\\\Lab 3\\\\Lab 3\\\\PostgreSQL-35-gis5572(postgres).sde'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create database connection\n",
    "dbname = 'PostgreSQL-35-gis5572(postgres).sde'\n",
    "platform = 'POSTGRESQL'\n",
    "user = 'postgres'\n",
    "password = 'Hyderabad43%'\n",
    "instance = '35.238.64.215'\n",
    "port = '5432'\n",
    "auth = 'DATABASE_AUTH'\n",
    "save = 'SAVE_USERNAME'\n",
    "db = 'gis5572'\n",
    "\n",
    "arcpy.management.CreateDatabaseConnection(\n",
    "    out_folder_path = file_path,\n",
    "    out_name = dbname,\n",
    "    database_platform = platform,\n",
    "    instance = instance,\n",
    "    account_authentication = auth,\n",
    "    username = user,\n",
    "    password = password,\n",
    "    save_user_pass = save,\n",
    "    database = db\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish variables for file and sde GDB\n",
    "file_gdb = os.path.join(file_path, 'Lab_3.gdb')\n",
    "sde_gdb = os.path.join(file_path, 'PostgreSQL-35-gis5572(postgres).sde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code below will be used to connect to database for weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Wednesday, April 10, 2024 2:16:47 PM\",\"Succeeded at Wednesday, April 10, 2024 2:16:50 PM (Elapsed Time: 2.82 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'C:\\\\Users\\\\15612\\\\Documents\\\\Arc II\\\\Lab 3\\\\Lab 3\\\\Lab_3.gdb\\\\mn_stations_20240410_sj'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the SDE feature class path\n",
    "sde_feature_class = \"gis5572.postgres.mn_stations_\" + today + \"_sj\"\n",
    "\n",
    "# Define the local feature class path\n",
    "local_feature_class = r\"C:\\Users\\15612\\Documents\\ArcGIS\\Projects\\ArcII Lab 2\\ArcII Lab 2.gdb\\mn_stations_\" + today + \"_sj\"\n",
    "\n",
    "# Process: Copy Features\n",
    "arcpy.CopyFeatures_management(r\"C:\\Users\\15612\\Documents\\Arc II\\Lab 3\\Lab 3\\PostgreSQL-35-gis5572(postgres).sde\\gis5572.postgres.mn_stations_\" + today + \"_sj\", os.path.join(file_gdb,\"mn_stations_\" + today + \"_sj\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50/50 Sampling\n",
    "\n",
    "The function below creates a random sample of a specified percentage of points the user inputs for further exploratory interpolation within the notebook. See below for documentation:\n",
    "\n",
    "### create_sample(input_feature_class {str}, x_field {str}, y_field {str}, z_field {str}, percent {float}, gdb_path {str}, out_feature_class {str})\n",
    "- input_feature_class - feature class where the randomly sampled points will be extracted from \n",
    "- id_field - field with ID of each feature\n",
    "- x_field - field with x coordinate\n",
    "- y_field - field with y coordinate\n",
    "- z_field - field with value to be interpolated \n",
    "- percent - ratio of points (in decimal notation) taken from original point feature class \n",
    "- gdb_path - path of geodatabase to save the created feature class to\n",
    "- out_feature_class - name of the output feature class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample(input_feature_class, id_field, x_field, y_field, z_field, percent, gdb_path, out_feature_class):\n",
    "    \n",
    "    print('Creating feature class \"' + out_feature_class + '\"...')\n",
    "    \n",
    "    # Create an empty list to add to\n",
    "    all_rows = []\n",
    "    \n",
    "    # Append the XYZ data to the empty list\n",
    "    with arcpy.da.SearchCursor(in_table = input_feature_class, field_names = [id_field,z_field, x_field, y_field]) as cursor:\n",
    "        for row in cursor:\n",
    "            name = row[0]\n",
    "            Z = row[1]\n",
    "            X = row[2]\n",
    "            Y = row[3]\n",
    "            all_rows.append([name, Z, X, Y])\n",
    "\n",
    "    # Establish variables for the number of total rows, the number of samples, and the number of rows outside of the sample rows\n",
    "    total_rows = len(all_rows)\n",
    "    sample_num = int(total_rows * percent)\n",
    "    removed_num = total_rows - sample_num\n",
    "\n",
    "    # Randomize the order of all the rows\n",
    "    random.shuffle(all_rows)\n",
    "    \n",
    "    # Create empty list for XYZ data\n",
    "    name_list = []\n",
    "    Z_list = []\n",
    "    X_list = []\n",
    "    Y_list = []\n",
    "\n",
    "    # Add XYZ data to separate lists\n",
    "    for row in all_rows:\n",
    "        name_list.append(row[0])\n",
    "        Z_list.append(row[1])\n",
    "        X_list.append(row[2])\n",
    "        Y_list.append(row[3])\n",
    "\n",
    "    # Remove all of the XYZ data which is not part of the sampled rows\n",
    "    df_list = [name_list, Z_list, X_list, Y_list]\n",
    "    for l in df_list:\n",
    "        del l[-removed_num:]\n",
    "\n",
    "    # Create a dictionary with all XYZ data\n",
    "    rand_dict = {\n",
    "        'ID':name_list,\n",
    "        'Z':Z_list,\n",
    "        'X':X_list,\n",
    "        'Y':Y_list\n",
    "    }\n",
    "\n",
    "    # Create a pandas dataframe from the dictionary\n",
    "    random_sample_df = pd.DataFrame(rand_dict)\n",
    "    \n",
    "    # Convert dataframe to sedf\n",
    "    sedf = arcgis.GeoAccessor.from_xy(\n",
    "        df = random_sample_df, \n",
    "        x_column = \"X\",\n",
    "        y_column = \"Y\"\n",
    "    )\n",
    "\n",
    "    # Convert sedf to feature class\n",
    "    sedf.spatial.to_featureclass(location=os.path.join(gdb_path, out_feature_class))\n",
    "    \n",
    "    # Define projection for the created feature class\n",
    "    arcpy.management.DefineProjection(\n",
    "        in_dataset = out_feature_class,\n",
    "        coor_system = spatial_ref\n",
    "    )\n",
    "    \n",
    "    # Change field type for Z values to 'Float'\n",
    "    arcpy.management.AddField(\n",
    "        in_table = out_feature_class,\n",
    "        field_name = z_field,\n",
    "        field_type = 'FLOAT'\n",
    "    )\n",
    "    \n",
    "    arcpy.management.CalculateField(\n",
    "        in_table = out_feature_class,\n",
    "        field = z_field,\n",
    "        expression = '!z!'\n",
    "    )\n",
    "    \n",
    "    arcpy.management.DeleteField(\n",
    "        in_table = out_feature_class,\n",
    "        drop_field = 'z'\n",
    "    )\n",
    "        \n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating feature class \"Random_Sample_50_temp\"...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Create a feature class with 50% of random points sampled\n",
    "create_sample(\n",
    "    input_feature_class = 'mn_stations_20240410_sj',\n",
    "    id_field = 'station_id',\n",
    "    x_field = 'longitude',\n",
    "    y_field = 'latitude',\n",
    "    z_field = 'max_daily_temp_f',\n",
    "    percent = 0.5,\n",
    "    gdb_path = file_gdb,\n",
    "    out_feature_class = 'Random_Sample_50_temp'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-One-Out Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Wednesday, April 10, 2024 2:22:40 PM\",\"Calculating Ordinary Kriging – Default\",\"\\r\\nWarning(s) for dataset: Random_Sample_50_temp\",\"WARNING 040402: NODATA value ignored. ObjectID = 37\",\"WARNING 040402: NODATA value ignored. ObjectID = 71\",\"Calculating Ordinary Kriging – Optimized\",\"\\r\\nWarning(s) for dataset: Random_Sample_50_temp\",\"WARNING 040402: NODATA value ignored. ObjectID = 37\",\"WARNING 040402: NODATA value ignored. ObjectID = 71\",\"Calculating Universal Kriging – Default\",\"\\r\\nWarning(s) for dataset: Random_Sample_50_temp\",\"WARNING 040402: NODATA value ignored. ObjectID = 37\",\"WARNING 040402: NODATA value ignored. ObjectID = 71\",\"Calculating Universal Kriging – Optimized\",\"\\r\\nWarning(s) for dataset: Random_Sample_50_temp\",\"WARNING 040402: NODATA value ignored. ObjectID = 37\",\"WARNING 040402: NODATA value ignored. ObjectID = 71\",\"Calculating Inverse Distance Weighted - Default\",\"\\r\\nWarning(s) for dataset: Random_Sample_50_temp\",\"WARNING 040402: NODATA value ignored. ObjectID = 37\",\"WARNING 040402: NODATA value ignored. ObjectID = 71\",\"Calculating Inverse Distance Weighted - Optimized\",\"\\r\\nWarning(s) for dataset: Random_Sample_50_temp\",\"WARNING 040402: NODATA value ignored. ObjectID = 37\",\"WARNING 040402: NODATA value ignored. ObjectID = 71\",\" \\n\",\"--------------------------------------------\",\"RANK | NAME\",\"--------------------------------------------\",\"\\n\",\"1    | Ordinary Kriging – Default\",\"\\n\",\"2    | Universal Kriging – Default\",\"\\n\",\"3    | Universal Kriging – Optimized\",\"\\n\",\"4    | Ordinary Kriging – Optimized\",\"\\n\",\"5    | Inverse Distance Weighted - Optimized\",\"\\n\",\"6    | Inverse Distance Weighted - Default\",\"--------------------------------------------\",\"Succeeded at Wednesday, April 10, 2024 2:22:43 PM (Elapsed Time: 2.70 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'C:\\\\Users\\\\15612\\\\Documents\\\\Arc II\\\\Lab 3\\\\Lab 3\\\\Lab_3.gdb\\\\stats_table_weather'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform exploratory interpolation on three different interpolation techniques using random sample\n",
    "arcpy.ga.ExploratoryInterpolation(\n",
    "    in_features = 'Random_Sample_50_temp',\n",
    "    value_field = 'max_daily_temp_F',\n",
    "    out_cv_table = os.path.join(file_gdb,'stats_table_weather'),\n",
    "    out_geostat_layer = None,\n",
    "    interp_methods = ['ORDINARY_KRIGING','UNIVERSAL_KRIGING','IDW'],\n",
    "    comparison_method = 'SINGLE',\n",
    "    criterion = 'ACCURACY',\n",
    "    criteria_hierarchy = 'ACCURACY PERCENT #',\n",
    "    weighted_criteria = 'ACCURACY 1',\n",
    "    exclusion_criteria = None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinary kriging\n",
    "temp_ord_krige = arcpy.sa.Kriging(\n",
    "    in_point_features = 'mn_stations_' + today + '_sj',\n",
    "    z_field = 'max_daily_temp_f',\n",
    "    kriging_model = arcpy.sa.KrigingModelOrdinary('SPHERICAL'),\n",
    "    cell_size = 0.139 # About 5 km in vertical degrees\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish extent as the size of the raster interpolating all weather station points\n",
    "arcpy.env.extent = 'temp_ord_krige'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinary kriging\n",
    "temp_ord_krige_sampled = arcpy.sa.Kriging(\n",
    "    in_point_features = 'Random_Sample_50_temp',\n",
    "    z_field = 'max_daily_temp_f',\n",
    "    kriging_model = arcpy.sa.KrigingModelOrdinary('SPHERICAL'),\n",
    "    cell_size = 0.139 # About 5 km in vertical degrees\n",
    ")\n",
    "\n",
    "# Universal kriging\n",
    "temp_uni_krige = arcpy.sa.Kriging(\n",
    "    in_point_features = 'mn_stations_' + today + '_sj',\n",
    "    z_field = 'max_daily_temp_f',\n",
    "    kriging_model = arcpy.sa.KrigingModelUniversal('LINEARDRIFT'),\n",
    "    cell_size = 0.139 # About 5 km in vertical degrees\n",
    ")\n",
    "\n",
    "temp_uni_krige_sampled = arcpy.sa.Kriging(\n",
    "    in_point_features = 'Random_Sample_50_temp',\n",
    "    z_field = 'max_daily_temp_f',\n",
    "    kriging_model = arcpy.sa.KrigingModelUniversal('LINEARDRIFT'),\n",
    "    cell_size = 0.139 # About 5 km in vertical degrees\n",
    ")\n",
    "\n",
    "# Inverse distance weighting\n",
    "temp_IDW = arcpy.sa.Idw(\n",
    "    in_point_features = 'mn_stations_' + today + '_sj',\n",
    "    z_field = 'max_daily_temp_f',\n",
    "    cell_size = 0.139 # About 5 km in vertical degrees\n",
    ")\n",
    "\n",
    "temp_IDW_sampled = arcpy.sa.Idw(\n",
    "    in_point_features = 'Random_Sample_50_temp',\n",
    "    z_field = 'max_daily_temp_f',\n",
    "    cell_size = 0.139 # About 5 km in vertical degrees\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform accuracy assessment\n",
    "The function below creates points from the sampled and actual rasters created via the interpolations above and compares their Z-values (in this case, daily maximum temperature). The difference between temperature for each point is added to the field named \"temp_difference\" in the output accuracy feature class. The average difference between each temperature is calculated and output as a simple \"print\" statement with the lowest value representing the interpolation method with the highest accuracy.\n",
    "\n",
    "### accuracy_analysis(sampled_interp_layer {str}, complete_interp_layer {str}, output_fc {str})\n",
    "- sampled_interp_layer - the raster layer created via interpolation of the sampled points\n",
    "- complete_interp_layer - the raster layer with the \"ground truth\" data\n",
    "- output_fc - the name of the output feature class with the accuracy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to perform accuracy analysis and add accuracy data to feature class\n",
    "def accuracy_analysis(sampled_interp_layer,complete_interp_layer,output_fc):\n",
    "\n",
    "    print('Performing accuracy analysis for \"' + sampled_interp_layer + '\" and \"' + complete_interp_layer + '\"...')\n",
    "    \n",
    "    # Create a list of each interpolation layer\n",
    "    interp_layer_list = [sampled_interp_layer,complete_interp_layer]\n",
    "    \n",
    "    # Iterate through each interpolation layer\n",
    "    for layer in interp_layer_list:\n",
    "    \n",
    "        # Turn interpolation data to points\n",
    "        arcpy.conversion.RasterToPoint(\n",
    "            in_raster = layer,\n",
    "            out_point_features = os.path.join(file_gdb,layer + '_points')\n",
    "        )\n",
    "        \n",
    "    # Spatially join all points\n",
    "    arcpy.analysis.SpatialJoin(\n",
    "        target_features = os.path.join(file_gdb,sampled_interp_layer + '_points'),\n",
    "        join_features = os.path.join(file_gdb,complete_interp_layer + '_points'),\n",
    "        out_feature_class = os.path.join(file_gdb,output_fc),\n",
    "        match_option = 'CLOSEST'\n",
    "    )\n",
    "    \n",
    "    # Add a field to add the difference values to\n",
    "    arcpy.management.AddField(\n",
    "        in_table = os.path.join(file_gdb,output_fc),\n",
    "        field_name = 'temp_difference',\n",
    "        field_type = 'FLOAT'\n",
    "    )\n",
    "    \n",
    "    # Calculate the difference between the actual and sampled weather station data\n",
    "    try:\n",
    "    \n",
    "        arcpy.management.CalculateField(\n",
    "            in_table = os.path.join(file_gdb,output_fc),\n",
    "            field = 'temp_difference',\n",
    "            expression = 'abs(!grid_code! - !grid_cod_1!)' \n",
    "        )\n",
    "        \n",
    "        arcpy.management.AlterField(\n",
    "            in_table = os.path.join(file_gdb,output_fc),\n",
    "            field = 'grid_cod_1',\n",
    "            new_field_name = 'actual_temp',\n",
    "            new_field_alias = 'actual_temp'\n",
    "        )\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        arcpy.management.CalculateField(\n",
    "            in_table = os.path.join(file_gdb,output_fc),\n",
    "            field = 'temp_difference',\n",
    "            expression = 'abs(!grid_code! - !grid_code_1!)' # Autocreates table with name as \"grid_cod_1\" or \"grid_code_1\"\n",
    "        )\n",
    "        \n",
    "        arcpy.management.AlterField(\n",
    "            in_table = os.path.join(file_gdb,output_fc),\n",
    "            field = 'grid_code_1',\n",
    "            new_field_name = 'actual_temp',\n",
    "            new_field_alias = 'actual_temp'\n",
    "        )\n",
    "    \n",
    "    # Change names of fields to represent data\n",
    "    arcpy.management.AlterField(\n",
    "        in_table = os.path.join(file_gdb,output_fc),\n",
    "        field = 'grid_code',\n",
    "        new_field_name = 'sampled_temp',\n",
    "        new_field_alias = 'sampled_temp'\n",
    "    )\n",
    "    \n",
    "    # Delete excess fields\n",
    "    arcpy.management.DeleteField(\n",
    "        in_table = os.path.join(file_gdb,output_fc),\n",
    "        drop_field = ['Join_Count','TARGET_FID','pointid_1']\n",
    "    )\n",
    "    \n",
    "    # Delete sampled interpolation raster\n",
    "    arcpy.management.Delete(\n",
    "        in_data = os.path.join(file_gdb,sampled_interp_layer + '_points')\n",
    "    )\n",
    "    \n",
    "    # Create empty list to append temperature difference values\n",
    "    temp_dif_list = []\n",
    "    \n",
    "    # Iterate through each row and collect values, then calculate average to find average difference of all values\n",
    "    with arcpy.da.SearchCursor(in_table = os.path.join(file_gdb,output_fc), field_names = 'temp_difference') as cursor:\n",
    "        for row in cursor:\n",
    "            temp_dif_list.append(row[0])\n",
    "            \n",
    "    print('The average temperature difference between \"' + sampled_interp_layer + '\" and \"' + complete_interp_layer + '\" is ' + str(sum(temp_dif_list)/len(temp_dif_list)))\n",
    "    \n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing accuracy analysis for \"temp_ord_krige_sampled\" and \"temp_ord_krige\"...\n",
      "The average temperature difference between \"temp_ord_krige_sampled\" and \"temp_ord_krige\" is 2.0567321858438503\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_analysis('temp_ord_krige_sampled','temp_ord_krige','temp_ord_krige_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing accuracy analysis for \"temp_uni_krige_sampled\" and \"temp_uni_krige\"...\n",
      "The average temperature difference between \"temp_uni_krige_sampled\" and \"temp_uni_krige\" is 2.8982729433345145\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_analysis('temp_uni_krige_sampled','temp_uni_krige','temp_uni_krige_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing accuracy analysis for \"temp_IDW_sampled\" and \"temp_IDW\"...\n",
      "The average temperature difference between \"temp_IDW_sampled\" and \"temp_IDW\" is 1.9753962811969576\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "accuracy_analysis('temp_IDW_sampled','temp_IDW','temp_IDW_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Inverse distance weighting (IDW) appears to have the smallest mean difference between point values, meaning it is likely the most accurate when observing temperature values. (It is important to note this was an exercise and the \"ground truth\" rasters are likely inaccurate in this case)\n",
    "\n",
    "The points created via IDW interpolation will be added to the SDE database in order to avoid confusion regarding which interpolation was most accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add each set of points created from raster to sde database\n",
    "points_list = [os.path.join(file_gdb,'temp_IDW_points'),os.path.join(file_gdb,'temp_IDW_accuracy')]\n",
    "\n",
    "for points in points_list:\n",
    "\n",
    "    arcpy.conversion.FeatureClassToGeodatabase(\n",
    "        Input_Features = points,\n",
    "        Output_Geodatabase = sde_gdb\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
